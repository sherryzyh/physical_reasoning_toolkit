# PhysKit Cookbooks

A collection of practical examples and tutorials for using PhysKit.

## ğŸš€ Quick Start

### Prerequisites
- PhysKit packages installed (`physkit_datasets`, `physkit_annotation`, `physkit_evaluation`)
- OpenAI API key set (for annotation and evaluation cookbooks)
- Dataset files available in your data directory

### Basic Usage
```bash
# Load and explore datasets
python 01_load_dataset.py ugphysics
python 01_load_dataset.py phybench
python 01_load_dataset.py physreason

# Run annotation workflows
python 02_automated_annotation.py


# Demo environment variables and workflow composition
python 03_environment_variables.py
python 04_workflow_composition_demo.py


# Test enhanced evaluation toolkit (25 comprehensive test scenarios)
python 05_answer_comparison_demo.py
```

## ğŸ“š Available Cookbooks

### 1. **Dataset Loading & Exploration** (`01_load_dataset.py`)
**Purpose**: Load, explore, and test any PhysKit dataset

**Features**:
- âœ… **Flexible**: Test any dataset by name (ugphysics, phybench, physreason, etc.)
- âœ… **Configurable**: Customize variant, split, sample size, and data directory
- âœ… **Comprehensive**: Shows dataset info, structure, sample problems, and statistics
- âœ… **Debug-friendly**: Detailed error messages and debugging output

**Usage Examples**:
```bash
# Test UGPhysics dataset (default)
python 01_load_dataset.py

# Test specific dataset
python 01_load_dataset.py phybench

# Test with custom parameters
python 01_load_dataset.py ugphysics --variant full --split test --sample-size 5

# Test with custom data directory
python 01_load_dataset.py physreason --data-dir ~/my_data
```

**Command Line Options**:
- `dataset_name`: Name of dataset to test (default: ugphysics)
- `--data-dir`: Data directory path (default: auto-detect)
- `--variant`: Dataset variant (default: full)
- `--split`: Dataset split (default: test)
- `--sample-size`: Number of problems to sample (default: all)

**Output**:
- Dataset information and statistics
- Sample problem structures
- Different loading options demonstration
- Saved files in `showcase_output/dataset_exploration/`

### 2. **Automated Annotation Workflow** (`02_automated_annotation.py`)
**Purpose**: Run unsupervised LLM-based annotation on physics problems

**Features**:
- âœ… **Sequential Pipeline**: Domain â†’ Theorem â†’ Variable â†’ Final Answer annotation
- âœ… **LLM Integration**: Uses OpenAI models for automated annotation
- âœ… **Configurable**: Sample sizes and model selection
- âœ… **Comprehensive Results**: Saves individual problem annotations and workflow statistics
- âœ… **Error Handling**: Graceful handling of failed annotations with detailed error reporting

**Usage**:
```bash
python 02_automated_annotation.py
```

**Prerequisites**:
- `OPENAI_API_KEY` environment variable set
- UGPhysics dataset available

**Output**:
- Individual problem annotation files in `annotation/` directory
- Workflow statistics and summary
- Detailed logging of each annotation step

### 3. **Environment Variables Demo** (`03_environment_variables.py`)
**Purpose**: Demonstrate environment variable configuration and priority

**Features**:
- Shows how to configure PhysKit with environment variables
- Demonstrates environment variable priority
- Configures API keys and settings
- Saves configuration examples

**Usage**:
```bash
python 03_environment_variables.py
```

### 4. **Workflow Composition Demo** (`04_workflow_composition_demo.py`)
**Purpose**: Demonstrate how to compose custom annotation workflows using WorkflowComposer

**Features**:
- âœ… **Custom Workflow Creation**: Build workflows by combining individual modules
- âœ… **Module Composition**: Add, remove, and chain workflow modules
- âœ… **Flexible Configuration**: Customize workflow parameters and settings
- âœ… **Result Analysis**: Comprehensive workflow statistics and data flow analysis
- âœ… **Status Monitoring**: Real-time workflow status and control capabilities

**Usage**:
```bash
python 04_workflow_composition_demo.py
```

**Prerequisites**:
- `OPENAI_API_KEY` environment variable set
- PHYBench dataset available

**Key Concepts Demonstrated**:
- WorkflowComposer for orchestration
- Module composition and chaining
- Result aggregation and statistics
- Output management and file organization

### 5. **Enhanced Evaluation Toolkit Demo** (`05_answer_comparison_demo.py`)
**Purpose**: Comprehensive demonstration of PhysKit's advanced evaluation capabilities across different answer types

**Features**:
- âœ… **Multi-type comparison**: Symbolic expressions, numerical values, textual descriptions, and multiple choice options
- âœ… **Advanced symbolic parsing**: Handles complex LaTeX, equations vs expressions, mathematical equivalence
- âœ… **Smart numerical comparison**: Significant figure-based comparison, unit conversions, special cases (infinity, NaN, zero)
- âœ… **Semantic textual analysis**: LLM-powered comparison for different phrasings and explanations
- âœ… **Intelligent option comparison**: Case-insensitive, order-independent multiple choice answer comparison
- âœ… **Comprehensive test scenarios**: 25 diverse physics problems covering various edge cases
- âœ… **Detailed analysis**: Per-type accuracy breakdown, comparison method details, error analysis

**Test Scenarios**:
- **Symbolic (5 problems)**: Complex velocity functions, Newton's laws, Einstein's E=mcÂ², integral equations
- **Numerical (8 problems)**: Unit conversions (km/h â†” m/s, Â°F â†” Â°C, g â†” kg), significant figures, special values
- **Textual (5 problems)**: Physics explanations with different phrasings and terminology
- **Option (7 problems)**: Single choice, multiple choice, case-insensitive, order-independent, different separators

**Usage**:
```bash
python 05_answer_comparison_demo.py
```

**Prerequisites**:
- `physkit_evaluation` package installed
- `physkit` package installed
- OpenAI API access for LLM-based comparisons

**Sample Output**:
```
ğŸ¯ Key Features Demonstrated:
  â€¢ Symbolic: Handles equations vs expressions, complex LaTeX parsing
  â€¢ Numerical: Unit conversions, significant figures, special cases (inf, NaN)
  â€¢ Textual: Semantic similarity using LLM comparison
  â€¢ Option: Case-insensitive, order-independent multiple choice comparison
  â€¢ Comprehensive error handling and detailed comparison results

ğŸ“ˆ Accuracy Breakdown by Answer Type
Symbolic:   80.00% (4/5)
Numerical:  75.00% (6/8)  
Textual:    100.00% (5/5)
Option:     100.00% (7/7)
```

**Advanced Capabilities Showcased**:
- **LaTeX Processing**: PhyBench preprocessing with `posify`, `time_simplify`, equation parsing
- **Unit Intelligence**: Dimensional analysis, automatic conversion factors, compatibility checking
- **Significant Figures**: Precision-aware comparison without fixed tolerance
- **LLM Integration**: GPT-4o for semantic comparison of units and explanations
- **Option Intelligence**: Smart multiple choice comparison with normalization, case-insensitive matching, order independence





## ğŸ“¥ Download Datasets

Before running the cookbooks, you need to download the required datasets. PhysKit currently supports three main datasets:

### **Supported Datasets**
- **UGPhysics**: Undergraduate physics problems across 13 domains
- **SeePhys**: Visual physics problems with images
- **PHYBench**: Physics benchmark dataset
- **PhysReason**: Physics reasoning problems with step-by-step solutions
- **JEEBench**: Challenging problems from IIT JEE-Advanced examination across Physics, Chemistry, and Mathematics

### **Dataset Structure**
Your datasets should be organized as follows:

```
~/data/                          # Your dataset root directory
â”œâ”€â”€ ugphysics/                   # UGPhysics dataset
â”‚   â”œâ”€â”€ AtomicPhysics/
â”‚   â”‚   â”œâ”€â”€ en.jsonl            # English problems
â”‚   â”‚   â””â”€â”€ zh.jsonl            # Chinese problems
â”‚   â”œâ”€â”€ ClassicalMechanics/
â”‚   â”‚   â”œâ”€â”€ en.jsonl
â”‚   â”‚   â””â”€â”€ zh.jsonl
â”‚   â”œâ”€â”€ QuantumMechanics/
â”‚   â”‚   â”œâ”€â”€ en.jsonl
â”‚   â”‚   â””â”€â”€ zh.jsonl
â”‚   â””â”€â”€ ...                     # 10 more domains
â”œâ”€â”€ SeePhys/                     # SeePhys dataset
â”‚   â”œâ”€â”€ physreason_format/      # Converted to PhysReason format
â”‚   â”‚   â”œâ”€â”€ 1900.json
â”‚   â”‚   â”œâ”€â”€ 1901.json
â”‚   â”‚   â””â”€â”€ ...                 # 2000+ problem files
â”‚   â”œâ”€â”€ images/                 # Problem images
â”‚   â”œâ”€â”€ seephys_train.csv      # Training data
â”‚   â””â”€â”€ seephys_train_mini.csv # Mini training data
â””â”€â”€ PHYBench/                   # PHYBench dataset
    â”œâ”€â”€ PHYBench-fullques_v1.json
    â”œâ”€â”€ PHYBench-onlyques_v1.json
    â””â”€â”€ PHYBench-questions_v1.json

```

### **Download Instructions**

#### **Option 1: Manual Download (Recommended)**
1. **UGPhysics**: Download from [Hugging Face Dataset](https://huggingface.co/datasets/UGPhysics/ugphysics)
2. **SeePhys**: Download from [SeePhys repository](https://github.com/AI4Phys/SeePhys?tab=readme-ov-file)
3. **PHYBench**: Download from [Hugging Face Dataset](https://huggingface.co/datasets/Eureka-Lab/PHYBench)

#### **Option 2: Custom Data Directory**
If you want to use a different directory structure:
```bash
# Set custom data directory when running cookbooks
python 01_load_dataset.py ugphysics --data-dir /path/to/your/datasets
```

### **Verification**
After downloading, verify your datasets:
```bash
# Test dataset loading
python 01_load_dataset.py ugphysics
python 01_load_dataset.py seephys
python 01_load_dataset.py phybench
```

## ğŸ”§ Setup & Configuration

### Environment Setup
```bash
# Set OpenAI API key
export OPENAI_API_KEY="your-api-key-here"

# Or create .env file
echo "OPENAI_API_KEY=your-api-key-here" > .env
```

### Quick Environment Check
```bash
# Run the setup script to check your environment
python setup_cookbooks.py
```

This will verify:
- Python version compatibility
- Required PhysKit packages
- OpenAI API key configuration
- Dataset availability

### Data Directory Structure
```
../data/
â”œâ”€â”€ ugphysics/           # UGPhysics dataset
â”‚   â”œâ”€â”€ mini/
â”‚   â”‚   â”œâ”€â”€ AtomicPhysics/
â”‚   â”‚   â”‚   â””â”€â”€ en.jsonl
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ full/
â”œâ”€â”€ phybench/            # PHYBench dataset
â”œâ”€â”€ physreason/          # PhysReason dataset
â””â”€â”€ ...                  # Other datasets
```

### Custom Data Paths
```bash
# Use custom data directory
python 01_load_dataset.py ugphysics --data-dir /path/to/your/data

# Test different variants
python 01_load_dataset.py ugphysics --variant full

# Test different splits
python 01_load_dataset.py phybench --split validation
```

## ğŸš¨ Troubleshooting

### Common Issues

**1. Dataset Not Found**
```bash
âŒ Dataset 'unknown_dataset' not found!
Available datasets: ugphysics, phybench, seephys
```
**Solution**: Use one of the available dataset names

**2. Data Directory Not Found**
```bash
âŒ Data directory not found: ../data
```
**Solution 1**: (recommended) Download datasets using the instructions in the [Download Datasets](#-download-datasets) section above
**Solution 2**: Create a "data/" folder under your home directory and create sub-folders with dataset names
**Solution 3**: Set correct data directory with `--data-dir` flag

**3. OpenAI API Key Missing**
```bash
âŒ OPENAI_API_KEY environment variable not set
```
**Solution**: Set `OPENAI_API_KEY` environment variable or create an `.env` file

**4. Evaluation Toolkit Import Error**
```bash
ModuleNotFoundError: No module named 'physkit_evaluation'
```
**Solution**: Ensure the `physkit_evaluation` package is properly installed and the import paths are correctly configured


## ğŸ“ Showcase Output Structure

```
showcase_output/
â”œâ”€â”€ dataset_exploration/
â”‚   â”œâ”€â”€ ugphysics_summary.txt
â”‚   â”œâ”€â”€ ugphysics_sample_problems.json
â”‚   â”œâ”€â”€ phybench_summary.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ automated_annotation/
â”‚   â”œâ”€â”€ annotation/
â”‚   â”‚   â”œâ”€â”€ problem_1.json
â”‚   â”‚   â””â”€â”€ problem_2.json
â”‚   â””â”€â”€ annotation_workflow.log
â”œâ”€â”€ workflow_composition_demo/
â”‚   â”œâ”€â”€ domain_assessment_demo_workflow_statistics.json
â”‚   â””â”€â”€ domain_assessment_demo_results.json
â””â”€â”€ evaluation_results/
    â”œâ”€â”€ symbolic_comparison_details.json
    â”œâ”€â”€ numerical_comparison_details.json
    â”œâ”€â”€ textual_comparison_details.json
    â””â”€â”€ comprehensive_evaluation_report.txt
```

---

**Happy cooking with PhysKit! ğŸ§ªâš¡**
